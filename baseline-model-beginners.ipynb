{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":112509,"databundleVersionId":14254895,"sourceType":"competition"}],"dockerImageVersionId":31154,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import necessary libraries\n\nImport all required packages for the biomass prediction model including PyTorch for deep learning, pandas for data handling, PIL for image processing, and scikit-learn for data splitting.\n\n**Note:** If Optuna is not installed, run: `pip install optuna optuna-dashboard` (or `!pip install optuna` in a notebook cell)\n","metadata":{}},{"cell_type":"markdown","source":"# Analyze Image Sizes in Dataset\n\nDefine a function to scan through image directories and analyze the distribution of image dimensions. This helps understand the dataset structure and ensures proper image sizing for the model. Displays results in a formatted table showing width, height, and quantity of each size.\n","metadata":{}},{"cell_type":"code","source":"# Install required packages from requirements.txt\n# This ensures all dependencies are installed before running the notebook\nimport sys\nimport subprocess\n\ndef install_requirements():\n    \"\"\"Install packages from requirements.txt\"\"\"\n    try:\n        # Try to read requirements.txt\n        with open('requirements.txt', 'r') as f:\n            requirements = f.read()\n        print(\"ğŸ“¦ Installing packages from requirements.txt...\")\n        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', '-r', 'requirements.txt'])\n        print(\"âœ… All packages installed successfully!\")\n    except FileNotFoundError:\n        pass\n    except Exception as e:\n        print(f\"âŒ Error installing packages: {e}\")\n        print(\"Please install packages manually or check your internet connection.\")\n\n# Install packages\ninstall_requirements()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T16:49:29.568038Z","iopub.execute_input":"2025-10-29T16:49:29.568277Z","execution_failed":"2025-10-29T16:50:50.888Z"}},"outputs":[{"name":"stdout","text":"âš ï¸  requirements.txt not found. Installing core packages individually...\n","output_type":"stream"},{"name":"stderr","text":"WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7aed703e0d10>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/nvidia-cuda-nvrtc-cu12/\nWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7aed703e19d0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/nvidia-cuda-nvrtc-cu12/\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"# Environment Configuration\n\nDetect whether running on Kaggle or locally and set up data paths accordingly.\n","metadata":{}},{"cell_type":"code","source":"# Detect environment and set data paths\nimport os\n\n# Check if running on Kaggle\nIS_KAGGLE = os.path.exists('/kaggle') or os.getenv('KAGGLE_KERNEL_TYPE') is not None\n\nif IS_KAGGLE:\n    # Kaggle paths\n    DATA_DIR = '/kaggle/input/csiro-biomass'\n    TRAIN_DIR = '/kaggle/input/csiro-biomass/train'\n    TEST_DIR = '/kaggle/input/csiro-biomass/test'\n    TRAIN_CSV = '/kaggle/input/csiro-biomass/train.csv'\n    TEST_CSV = '/kaggle/input/csiro-biomass/test.csv'\n    print(\"ğŸ”µ Running on Kaggle\")\nelse:\n    # Local paths - adjust these to match your local data structure\n    # Assuming data is in ./data/ directory relative to notebook\n    DATA_DIR = '.'\n    TRAIN_DIR = './train'\n    TEST_DIR = './test'\n    TRAIN_CSV = './train.csv'\n    TEST_CSV = './test.csv'\n    print(\"ğŸŸ¢ Running locally\")\n    \nprint(f\"Data directory: {DATA_DIR}\")\nprint(f\"Train CSV: {TRAIN_CSV}\")\nprint(f\"Test CSV: {TEST_CSV}\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-29T16:50:50.888Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nimport torchvision.transforms as transforms\nimport optuna\n\nfrom PIL import Image\nfrom collections import defaultdict\nfrom torchvision.models import resnet34\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"execution_failed":"2025-10-29T16:50:50.889Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load training data\n\nLoad the training CSV file which contains metadata about the images including image paths, target values, and target names (biomass types).\n","metadata":{}},{"cell_type":"code","source":"def get_unique_sizes(directory):\n    size_counts = defaultdict(int)\n    if not os.path.exists(directory):\n        return size_counts\n    for root, _, files in os.walk(directory):\n        for file in files:\n            if file.lower().endswith(('.png', '.jpg', '.jpeg', 'JPG')):\n                try:\n                    with Image.open(os.path.join(root, file)) as img:\n                        size = img.size\n                        size_counts[size] += 1\n                except Exception as e:\n                    print(f\"Error {file}: {e}\")\n\n    return size_counts\n\nfolders = [\n    TRAIN_DIR,\n    TEST_DIR,\n]\n\nfor folder in folders:\n    print(f\"\\nğŸ“‚ Folder: {folder}\")\n    if not os.path.exists(folder):\n        print(f\"âš ï¸  Directory does not exist: {folder}\")\n        continue\n    sizes = get_unique_sizes(folder)\n\n    if not sizes:\n        print(\"No images or mistake in code\")\n        continue\n    \n    sorted_sizes = sorted(sizes.items(), key=lambda x: x[1], reverse=True)\n\n    print(\"â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\")\n    print(\"â”‚  Width (px)  â”‚ Height (px)  â”‚Quantityâ”‚\")\n    print(\"â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\")\n    for (w, h), count in sorted_sizes:\n        print(f\"â”‚ {w:<13} â”‚ {h:<13} â”‚ {count:<7} â”‚\")\n    print(\"â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"execution_failed":"2025-10-29T16:50:50.889Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Custom Dataset Class and Data Transforms\n\nDefine a custom PyTorch Dataset class that:\n- Loads images from paths in the dataframe\n- Applies transformations (resizing, augmentation for training)\n- Maps different target types (biomass categories) to indices\n- Returns images with their corresponding target values and types\n\nAlso defines two transform pipelines:\n- `train_transform`: Includes data augmentation (random flips, color jitter) for training\n- `val_transform`: Only resizing and normalization for validation/testing\n","metadata":{}},{"cell_type":"markdown","source":"# Model Architecture: Multi-Head ResNet34\n\nDefine a ResNet34-based model with multiple output heads (one for each target type):\n- Uses ResNet34 as the feature extraction backbone\n- Extracts shared features through fully connected layers\n- Has 5 separate heads (one for each biomass type: Dry_Green_g, Dry_Dead_g, Dry_Clover_g, GDM_g, Dry_Total_g)\n- Each head outputs a single value (regression prediction)\n- The model selects which head to use based on the target_type during training\n","metadata":{}},{"cell_type":"markdown","source":"# Training Function\n\nDefine the model training loop that:\n- Trains the model for specified number of epochs\n- Iterates through training batches, computes loss, and updates weights\n- Evaluates on validation set after each epoch\n- Tracks and returns training and validation losses\n- Uses the appropriate model head based on target_type for each sample\n","metadata":{}},{"cell_type":"markdown","source":"# Split Data into Train and Validation Sets\n\nSplit the training data into train (80%) and validation (20%) sets while maintaining stratification by target_name to ensure balanced distribution of biomass types in both sets.\n","metadata":{}},{"cell_type":"markdown","source":"# Create Datasets and Data Loaders\n\nInstantiate the training and validation datasets with their respective transforms, then create DataLoader objects for efficient batch loading during training. Also determine and set the computation device (GPU if available, else CPU).\n","metadata":{}},{"cell_type":"markdown","source":"# Initialize Model and Train\n\nCreate the ResNet34 model instance, define loss function (MSE for regression), optimizer (Adam), and learning rate scheduler. Then train the model for the specified number of epochs.\n","metadata":{}},{"cell_type":"markdown","source":"# Visualize Training History\n\nPlot the training and validation loss curves to visualize model performance over epochs and check for overfitting or convergence.\n","metadata":{}},{"cell_type":"markdown","source":"# Generate Predictions on Test Set\n\nLoad the test dataset and make predictions:\n- Load test CSV and create test dataset with validation transforms\n- Run inference on test images\n- For each sample, select the appropriate output head based on the target_name\n- Collect predictions and corresponding sample IDs\n","metadata":{}},{"cell_type":"markdown","source":"# Create Submission File\n\nFormat the predictions into a submission CSV file with sample_id and target columns, and save it for competition submission.\n","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(TRAIN_CSV)\ntrain.head()","metadata":{"execution":{"execution_failed":"2025-10-29T16:50:50.889Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DatasetCS(Dataset):\n    def __init__(self, df, images_dir, transform=None, is_test=False):\n        self.df = df\n        self.images_dir = images_dir\n        self.transform = transform\n        self.is_test = is_test\n        \n        if not is_test:\n            self.target_mapping = {\n                'Dry_Green_g': 0, 'Dry_Dead_g': 1, 'Dry_Clover_g': 2,\n                'GDM_g': 3, 'Dry_Total_g': 4\n            }\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        image_path = os.path.join(self.images_dir, row['image_path'])\n        \n        image = Image.open(image_path).convert('RGB')\n        \n        if self.transform:\n            image = self.transform(image)\n        \n        if self.is_test:\n            return image, row['sample_id']\n        else:\n            target_value = row['target']\n            target_type = self.target_mapping[row['target_name']]\n            return image, torch.tensor(target_value, dtype=torch.float32), target_type\n\ntrain_transform = transforms.Compose([\n    transforms.Resize((500, 250)),\n    transforms.RandomHorizontalFlip(p=0.3),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\nval_transform = transforms.Compose([\n    transforms.Resize((500, 250)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])","metadata":{"execution":{"execution_failed":"2025-10-29T16:50:50.889Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ResNet34(nn.Module):\n    def __init__(self, num_targets=5, hidden1=512, hidden2=256, head_hidden=128, \n                 dropout_shared=0.3, dropout_head=0.2):\n        super(ResNet34, self).__init__()\n        self.backbone = resnet34(weights=None)\n        self.backbone.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        \n        in_features = self.backbone.fc.in_features\n        self.backbone.fc = nn.Identity()\n        \n        self.shared_features = nn.Sequential(\n            nn.Linear(in_features, hidden1),\n            nn.BatchNorm1d(hidden1),\n            nn.ReLU(inplace=True),\n            nn.Dropout(dropout_shared),\n            nn.Linear(hidden1, hidden2),\n            nn.BatchNorm1d(hidden2),\n            nn.ReLU(inplace=True),\n        )\n        \n        self.heads = nn.ModuleList([\n            nn.Sequential(\n                nn.Dropout(dropout_head),\n                nn.Linear(hidden2, head_hidden),\n                nn.ReLU(inplace=True),\n                nn.Linear(head_hidden, 1)\n            ) for _ in range(num_targets)\n        ])\n        \n        self._initialize_weights()\n    \n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0, 0.01)\n                nn.init.constant_(m.bias, 0)\n    \n    def forward(self, x, target_type=None):\n        features = self.backbone(x)\n        shared_out = self.shared_features(features)\n        \n        if target_type is not None:\n            outputs = []\n            for i, t_type in enumerate(target_type):\n                outputs.append(self.heads[t_type](shared_out[i].unsqueeze(0)))\n            return torch.cat(outputs, dim=0)\n        else:\n            all_outputs = [head(shared_out) for head in self.heads]\n            return torch.cat(all_outputs, dim=1)","metadata":{"execution":{"execution_failed":"2025-10-29T16:50:50.889Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=15, \n                scheduler=None, verbose=True, return_best_val_loss=False):\n    train_losses = []\n    val_losses = []\n    best_val_loss = float('inf')\n    \n    for epoch in range(num_epochs):\n        model.train()\n        train_loss = 0.0\n        \n        for images, targets, target_types in train_loader:\n            images = images.to(device)\n            targets = targets.to(device)\n            target_types = target_types.to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(images, target_types).squeeze()\n            loss = criterion(outputs, targets)\n            loss.backward()\n            optimizer.step()\n            \n            train_loss += loss.item()\n        \n        model.eval()\n        val_loss = 0.0\n        \n        with torch.no_grad():\n            for images, targets, target_types in val_loader:\n                images = images.to(device)\n                targets = targets.to(device)\n                target_types = target_types.to(device)\n                \n                outputs = model(images, target_types).squeeze()\n                loss = criterion(outputs, targets)\n                val_loss += loss.item()\n        \n        train_loss /= len(train_loader)\n        val_loss /= len(val_loader)\n        train_losses.append(train_loss)\n        val_losses.append(val_loss)\n        \n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n        \n        if scheduler is not None:\n            scheduler.step(val_loss)\n        \n        if verbose:\n            print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n    \n    if return_best_val_loss:\n        return train_losses, val_losses, best_val_loss\n    return train_losses, val_losses","metadata":{"execution":{"execution_failed":"2025-10-29T16:50:50.889Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_indices, val_indices = train_test_split(\n    range(len(train)), \n    test_size=0.2, \n    random_state=42, \n    stratify=train['target_name']\n)\n\ntrain_subset = train.iloc[train_indices].reset_index(drop=True)\nval_subset = train.iloc[val_indices].reset_index(drop=True)\n\nprint(f\"train length: {len(train_subset)}\")\nprint(f\"val length: {len(val_subset)}\")","metadata":{"execution":{"execution_failed":"2025-10-29T16:50:50.889Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = DatasetCS(train_subset, DATA_DIR, transform=train_transform)\nval_dataset = DatasetCS(val_subset, DATA_DIR, transform=val_transform)\n\n# Adjust num_workers for local runs (0 for local to avoid multiprocessing issues, 2 for Kaggle)\n# Using 0 for local runs avoids DataLoader worker errors on macOS/Windows\nnum_workers = 0 if not IS_KAGGLE else 2\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=num_workers)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=num_workers)\n\n# Device configuration: prioritize GPU on Kaggle\nif IS_KAGGLE:\n    if torch.cuda.is_available():\n        device = torch.device('cuda')\n        print(f\"âœ… GPU available: Using {device}\")\n        print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n        print(f\"   CUDA Version: {torch.version.cuda}\")\n    else:\n        device = torch.device('cpu')\n        print(\"âš ï¸  WARNING: GPU not available on Kaggle!\")\n        print(\"   Please enable GPU in Kaggle Notebook Settings (Settings â†’ Accelerator â†’ GPU)\")\n        print(f\"   Falling back to: {device}\")\nelse:\n    # Local: use GPU if available, otherwise CPU\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n    if torch.cuda.is_available():\n        print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n\ndevice","metadata":{"execution":{"execution_failed":"2025-10-29T16:50:50.889Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Hyperparameter Optimization with Optuna\n\nDefine an objective function for Optuna that creates and trains a model with different hyperparameters. Optuna will search for the best combination of learning rate, batch size, hidden layer sizes, dropout rates, and optimizer settings.\n","metadata":{}},{"cell_type":"code","source":"def objective(trial):\n    \"\"\"Optuna objective function to optimize hyperparameters\"\"\"\n    \n    # Suggest hyperparameters\n    lr = trial.suggest_float('lr', 1e-5, 1e-2, log=True)\n    batch_size = trial.suggest_categorical('batch_size', [8, 16, 32])\n    hidden1 = trial.suggest_categorical('hidden1', [256, 512, 768])\n    hidden2 = trial.suggest_categorical('hidden2', [128, 256, 512])\n    head_hidden = trial.suggest_categorical('head_hidden', [64, 128, 256])\n    dropout_shared = trial.suggest_float('dropout_shared', 0.1, 0.5)\n    dropout_head = trial.suggest_float('dropout_head', 0.1, 0.4)\n    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'AdamW', 'RMSprop'])\n    \n    # Adjust num_workers for local runs (0 for local to avoid multiprocessing issues, 2 for Kaggle)\n    num_workers_opt = 0 if not IS_KAGGLE else 2\n    \n    # Create data loaders with new batch size\n    train_loader_opt = DataLoader(\n        train_dataset, \n        batch_size=batch_size, \n        shuffle=True, \n        num_workers=num_workers_opt\n    )\n    val_loader_opt = DataLoader(\n        val_dataset, \n        batch_size=batch_size, \n        shuffle=False, \n        num_workers=num_workers_opt\n    )\n    \n    # Create model with hyperparameters\n    model = ResNet34(\n        num_targets=5,\n        hidden1=hidden1,\n        hidden2=hidden2,\n        head_hidden=head_hidden,\n        dropout_shared=dropout_shared,\n        dropout_head=dropout_head\n    ).to(device)\n    \n    # Create optimizer\n    if optimizer_name == 'Adam':\n        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    elif optimizer_name == 'AdamW':\n        optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n    else:\n        optimizer = torch.optim.RMSprop(model.parameters(), lr=lr)\n    \n    criterion = nn.MSELoss()\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, patience=2, factor=0.5\n    )\n    \n    # Train for a few epochs (fewer for optimization)\n    _, _, best_val_loss = train_model(\n        model,\n        train_loader_opt,\n        val_loader_opt,\n        criterion,\n        optimizer,\n        num_epochs=5,  # Reduced epochs for faster optimization\n        scheduler=scheduler,\n        verbose=False,\n        return_best_val_loss=True\n    )\n    \n    return best_val_loss\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-29T16:50:50.889Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Run Hyperparameter Optimization\n\nCreate an Optuna study and run optimization trials. This will search for the best hyperparameters by training multiple models with different configurations and comparing their validation losses.\n","metadata":{}},{"cell_type":"code","source":"# Create Optuna study\nstudy = optuna.create_study(\n    direction='minimize',\n    study_name='biomass_optimization',\n    pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=3)\n)\n\n# Run optimization (adjust n_trials based on available time)\nprint(\"Starting hyperparameter optimization...\")\nstudy.optimize(objective, n_trials=20, show_progress_bar=True)\n\nprint(\"\\nBest trial:\")\ntrial = study.best_trial\nprint(f\"  Value (best validation loss): {trial.value:.4f}\")\nprint(\"\\n  Params:\")\nfor key, value in trial.params.items():\n    print(f\"    {key}: {value}\")\n    \n# Store best parameters\nbest_params = trial.params\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-29T16:50:50.889Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Visualize Optimization Results\n\nPlot the optimization history and parameter importance to understand which hyperparameters have the most impact on model performance.\n","metadata":{}},{"cell_type":"code","source":"# Plot optimization history\nfig1 = optuna.visualization.plot_optimization_history(study)\nfig1.show()\n\n# Plot parameter importance\nfig2 = optuna.visualization.plot_param_importances(study)\nfig2.show()\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-29T16:50:50.889Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train Final Model with Best Hyperparameters\n\nTrain the model using the optimized hyperparameters found by Optuna. This should yield better performance than the manually selected hyperparameters.\n","metadata":{}},{"cell_type":"code","source":"# Create data loaders with best batch size\nbest_batch_size = best_params['batch_size']\n# Adjust num_workers for local runs\nnum_workers_best = 0 if not IS_KAGGLE else 2\ntrain_loader_best = DataLoader(\n    train_dataset, \n    batch_size=best_batch_size, \n    shuffle=True, \n    num_workers=num_workers_best\n)\nval_loader_best = DataLoader(\n    val_dataset, \n    batch_size=best_batch_size, \n    shuffle=False, \n    num_workers=num_workers_best\n)\n\n# Create model with best hyperparameters\nmodel_best = ResNet34(\n    num_targets=5,\n    hidden1=best_params['hidden1'],\n    hidden2=best_params['hidden2'],\n    head_hidden=best_params['head_hidden'],\n    dropout_shared=best_params['dropout_shared'],\n    dropout_head=best_params['dropout_head']\n).to(device)\n\n# Create optimizer with best learning rate\nif best_params['optimizer'] == 'Adam':\n    optimizer_best = torch.optim.Adam(model_best.parameters(), lr=best_params['lr'])\nelif best_params['optimizer'] == 'AdamW':\n    optimizer_best = torch.optim.AdamW(model_best.parameters(), lr=best_params['lr'])\nelse:\n    optimizer_best = torch.optim.RMSprop(model_best.parameters(), lr=best_params['lr'])\n\ncriterion = nn.MSELoss()\nscheduler_best = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer_best, patience=3, factor=0.5\n)\n\n# Train with best hyperparameters\ntrain_losses_best, val_losses_best = train_model(\n    model_best,\n    train_loader_best,\n    val_loader_best,\n    criterion,\n    optimizer_best,\n    num_epochs=15,  # Full training with best hyperparameters\n    scheduler=scheduler_best\n)\n\n# Store the best model for inference\nmodel = model_best\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-29T16:50:50.889Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Original training without optimization (for comparison)\n# Uncomment below to train with default hyperparameters instead of optimized ones\n\n# model = ResNet34(num_targets=5).to(device)\n# criterion = nn.MSELoss()\n# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, factor=0.5)\n# train_losses, val_losses = train_model(\n#     model,\n#     train_loader,\n#     val_loader,\n#     criterion,\n#     optimizer,\n#     num_epochs=3\n# )","metadata":{"execution":{"execution_failed":"2025-10-29T16:50:50.889Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(10, 4))\n\nplt.plot(train_losses_best, label='Train Loss (Optimized)')\nplt.plot(val_losses_best, label='Val Loss (Optimized)')\n\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training History with Optimized Hyperparameters')\n\nplt.legend()\nplt.grid(True)\nplt.show()","metadata":{"execution":{"execution_failed":"2025-10-29T16:50:50.889Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test = pd.read_csv(TEST_CSV)\ntest_dataset = DatasetCS(test, DATA_DIR, transform=val_transform, is_test=True)\n# Adjust num_workers for local runs (0 for local to avoid multiprocessing issues, 2 for Kaggle)\nnum_workers = 0 if not IS_KAGGLE else 2\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=num_workers)\n\nmodel.eval()\npredictions = []\nsample_ids = []\n\ntarget_mapping = {\n    'Dry_Green_g': 0, 'Dry_Dead_g': 1, 'Dry_Clover_g': 2,\n    'GDM_g': 3, 'Dry_Total_g': 4\n}\n\nwith torch.no_grad():\n    for images, batch_sample_ids in test_loader:\n        images = images.to(device)\n        batch_outputs = model(images)\n        \n        for i, sample_id in enumerate(batch_sample_ids):\n            row = test[test['sample_id'] == sample_id].iloc[0]\n            target_idx = target_mapping[row['target_name']]\n            prediction = batch_outputs[i, target_idx].item()\n            predictions.append(prediction)\n            sample_ids.append(sample_id)","metadata":{"execution":{"execution_failed":"2025-10-29T16:50:50.889Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission = pd.DataFrame({\n    'sample_id': sample_ids,\n    'target': predictions\n})\n\nsubmission.to_csv('submission.csv', index=False)\nsubmission","metadata":{"execution":{"execution_failed":"2025-10-29T16:50:50.889Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}