{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "Import all required packages for the biomass prediction model including PyTorch for deep learning, pandas for data handling, PIL for image processing, and scikit-learn for data splitting.\n",
    "\n",
    "**Note:** If Optuna is not installed, run: `pip install optuna optuna-dashboard` (or `!pip install optuna` in a notebook cell)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Image Sizes in Dataset\n",
    "\n",
    "Define a function to scan through image directories and analyze the distribution of image dimensions. This helps understand the dataset structure and ensures proper image sizing for the model. Displays results in a formatted table showing width, height, and quantity of each size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Installing packages from requirements.txt...\n",
      "✅ All packages installed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Install required packages from requirements.txt\n",
    "# This ensures all dependencies are installed before running the notebook\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def install_requirements():\n",
    "    \"\"\"Install packages from requirements.txt\"\"\"\n",
    "    try:\n",
    "        # Try to read requirements.txt\n",
    "        with open('requirements.txt', 'r') as f:\n",
    "            requirements = f.read()\n",
    "        print(\"📦 Installing packages from requirements.txt...\")\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', '-r', 'requirements.txt'])\n",
    "        print(\"✅ All packages installed successfully!\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"⚠️  requirements.txt not found. Installing core packages individually...\")\n",
    "        packages = [\n",
    "            'torch>=2.0.0',\n",
    "            'torchvision>=0.15.0',\n",
    "            'numpy>=1.21.0',\n",
    "            'pandas>=1.3.0',\n",
    "            'Pillow>=9.0.0',\n",
    "            'scikit-learn>=1.0.0',\n",
    "            'matplotlib>=3.5.0',\n",
    "            'seaborn>=0.11.0',\n",
    "            'optuna>=3.0.0'\n",
    "        ]\n",
    "        for package in packages:\n",
    "            subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', package])\n",
    "        print(\"✅ Core packages installed!\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error installing packages: {e}\")\n",
    "        print(\"Please install packages manually or check your internet connection.\")\n",
    "\n",
    "# Install packages\n",
    "install_requirements()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Configuration\n",
    "\n",
    "Detect whether running on Kaggle or locally and set up data paths accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🟢 Running locally\n",
      "Data directory: .\n",
      "Train CSV: ./train.csv\n",
      "Test CSV: ./test.csv\n"
     ]
    }
   ],
   "source": [
    "# Detect environment and set data paths\n",
    "import os\n",
    "\n",
    "# Check if running on Kaggle\n",
    "IS_KAGGLE = os.path.exists('/kaggle') or os.getenv('KAGGLE_KERNEL_TYPE') is not None\n",
    "\n",
    "if IS_KAGGLE:\n",
    "    # Kaggle paths\n",
    "    DATA_DIR = '/kaggle/input/csiro-biomass'\n",
    "    TRAIN_DIR = '/kaggle/input/csiro-biomass/train'\n",
    "    TEST_DIR = '/kaggle/input/csiro-biomass/test'\n",
    "    TRAIN_CSV = '/kaggle/input/csiro-biomass/train.csv'\n",
    "    TEST_CSV = '/kaggle/input/csiro-biomass/test.csv'\n",
    "    print(\"🔵 Running on Kaggle\")\n",
    "else:\n",
    "    # Local paths - adjust these to match your local data structure\n",
    "    # Assuming data is in ./data/ directory relative to notebook\n",
    "    DATA_DIR = '.'\n",
    "    TRAIN_DIR = './train'\n",
    "    TEST_DIR = './test'\n",
    "    TRAIN_CSV = './train.csv'\n",
    "    TEST_CSV = './test.csv'\n",
    "    print(\"🟢 Running locally\")\n",
    "    \n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Train CSV: {TRAIN_CSV}\")\n",
    "print(f\"Test CSV: {TEST_CSV}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T09:05:24.754198Z",
     "iopub.status.busy": "2025-10-29T09:05:24.753538Z",
     "iopub.status.idle": "2025-10-29T09:05:24.760844Z",
     "shell.execute_reply": "2025-10-29T09:05:24.759754Z",
     "shell.execute_reply.started": "2025-10-29T09:05:24.754167Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/martonrausch/Documents/biomass/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "import optuna\n",
    "\n",
    "from PIL import Image\n",
    "from collections import defaultdict\n",
    "from torchvision.models import resnet34\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load training data\n",
    "\n",
    "Load the training CSV file which contains metadata about the images including image paths, target values, and target names (biomass types).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-29T09:05:24.762538Z",
     "iopub.status.busy": "2025-10-29T09:05:24.762206Z",
     "iopub.status.idle": "2025-10-29T09:05:25.094531Z",
     "shell.execute_reply": "2025-10-29T09:05:25.093593Z",
     "shell.execute_reply.started": "2025-10-29T09:05:24.762517Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📂 Folder: ./train\n",
      "┌───────────────┬───────────────┬─────────┐\n",
      "│  Width (px)  │ Height (px)  │Quantity│\n",
      "├───────────────┼───────────────┼─────────┤\n",
      "│ 2000          │ 1000          │ 357     │\n",
      "└───────────────┴───────────────┴─────────┘\n",
      "\n",
      "📂 Folder: ./test\n",
      "┌───────────────┬───────────────┬─────────┐\n",
      "│  Width (px)  │ Height (px)  │Quantity│\n",
      "├───────────────┼───────────────┼─────────┤\n",
      "│ 2000          │ 1000          │ 1       │\n",
      "└───────────────┴───────────────┴─────────┘\n"
     ]
    }
   ],
   "source": [
    "def get_unique_sizes(directory):\n",
    "    size_counts = defaultdict(int)\n",
    "    if not os.path.exists(directory):\n",
    "        return size_counts\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.png', '.jpg', '.jpeg', 'JPG')):\n",
    "                try:\n",
    "                    with Image.open(os.path.join(root, file)) as img:\n",
    "                        size = img.size\n",
    "                        size_counts[size] += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"Error {file}: {e}\")\n",
    "\n",
    "    return size_counts\n",
    "\n",
    "folders = [\n",
    "    TRAIN_DIR,\n",
    "    TEST_DIR,\n",
    "]\n",
    "\n",
    "for folder in folders:\n",
    "    print(f\"\\n📂 Folder: {folder}\")\n",
    "    if not os.path.exists(folder):\n",
    "        print(f\"⚠️  Directory does not exist: {folder}\")\n",
    "        continue\n",
    "    sizes = get_unique_sizes(folder)\n",
    "\n",
    "    if not sizes:\n",
    "        print(\"No images or mistake in code\")\n",
    "        continue\n",
    "    \n",
    "    sorted_sizes = sorted(sizes.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    print(\"┌───────────────┬───────────────┬─────────┐\")\n",
    "    print(\"│  Width (px)  │ Height (px)  │Quantity│\")\n",
    "    print(\"├───────────────┼───────────────┼─────────┤\")\n",
    "    for (w, h), count in sorted_sizes:\n",
    "        print(f\"│ {w:<13} │ {h:<13} │ {count:<7} │\")\n",
    "    print(\"└───────────────┴───────────────┴─────────┘\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Dataset Class and Data Transforms\n",
    "\n",
    "Define a custom PyTorch Dataset class that:\n",
    "- Loads images from paths in the dataframe\n",
    "- Applies transformations (resizing, augmentation for training)\n",
    "- Maps different target types (biomass categories) to indices\n",
    "- Returns images with their corresponding target values and types\n",
    "\n",
    "Also defines two transform pipelines:\n",
    "- `train_transform`: Includes data augmentation (random flips, color jitter) for training\n",
    "- `val_transform`: Only resizing and normalization for validation/testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture: Multi-Head ResNet34\n",
    "\n",
    "Define a ResNet34-based model with multiple output heads (one for each target type):\n",
    "- Uses ResNet34 as the feature extraction backbone\n",
    "- Extracts shared features through fully connected layers\n",
    "- Has 5 separate heads (one for each biomass type: Dry_Green_g, Dry_Dead_g, Dry_Clover_g, GDM_g, Dry_Total_g)\n",
    "- Each head outputs a single value (regression prediction)\n",
    "- The model selects which head to use based on the target_type during training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Function\n",
    "\n",
    "Define the model training loop that:\n",
    "- Trains the model for specified number of epochs\n",
    "- Iterates through training batches, computes loss, and updates weights\n",
    "- Evaluates on validation set after each epoch\n",
    "- Tracks and returns training and validation losses\n",
    "- Uses the appropriate model head based on target_type for each sample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data into Train and Validation Sets\n",
    "\n",
    "Split the training data into train (80%) and validation (20%) sets while maintaining stratification by target_name to ensure balanced distribution of biomass types in both sets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Datasets and Data Loaders\n",
    "\n",
    "Instantiate the training and validation datasets with their respective transforms, then create DataLoader objects for efficient batch loading during training. Also determine and set the computation device (GPU if available, else CPU).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Model and Train\n",
    "\n",
    "Create the ResNet34 model instance, define loss function (MSE for regression), optimizer (Adam), and learning rate scheduler. Then train the model for the specified number of epochs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Training History\n",
    "\n",
    "Plot the training and validation loss curves to visualize model performance over epochs and check for overfitting or convergence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Predictions on Test Set\n",
    "\n",
    "Load the test dataset and make predictions:\n",
    "- Load test CSV and create test dataset with validation transforms\n",
    "- Run inference on test images\n",
    "- For each sample, select the appropriate output head based on the target_name\n",
    "- Collect predictions and corresponding sample IDs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Submission File\n",
    "\n",
    "Format the predictions into a submission CSV file with sample_id and target columns, and save it for competition submission.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T09:05:25.095578Z",
     "iopub.status.busy": "2025-10-29T09:05:25.095309Z",
     "iopub.status.idle": "2025-10-29T09:05:25.113935Z",
     "shell.execute_reply": "2025-10-29T09:05:25.113165Z",
     "shell.execute_reply.started": "2025-10-29T09:05:25.095553Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>image_path</th>\n",
       "      <th>Sampling_Date</th>\n",
       "      <th>State</th>\n",
       "      <th>Species</th>\n",
       "      <th>Pre_GSHH_NDVI</th>\n",
       "      <th>Height_Ave_cm</th>\n",
       "      <th>target_name</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID1011485656__Dry_Clover_g</td>\n",
       "      <td>train/ID1011485656.jpg</td>\n",
       "      <td>2015/9/4</td>\n",
       "      <td>Tas</td>\n",
       "      <td>Ryegrass_Clover</td>\n",
       "      <td>0.62</td>\n",
       "      <td>4.6667</td>\n",
       "      <td>Dry_Clover_g</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID1011485656__Dry_Dead_g</td>\n",
       "      <td>train/ID1011485656.jpg</td>\n",
       "      <td>2015/9/4</td>\n",
       "      <td>Tas</td>\n",
       "      <td>Ryegrass_Clover</td>\n",
       "      <td>0.62</td>\n",
       "      <td>4.6667</td>\n",
       "      <td>Dry_Dead_g</td>\n",
       "      <td>31.9984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID1011485656__Dry_Green_g</td>\n",
       "      <td>train/ID1011485656.jpg</td>\n",
       "      <td>2015/9/4</td>\n",
       "      <td>Tas</td>\n",
       "      <td>Ryegrass_Clover</td>\n",
       "      <td>0.62</td>\n",
       "      <td>4.6667</td>\n",
       "      <td>Dry_Green_g</td>\n",
       "      <td>16.2751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID1011485656__Dry_Total_g</td>\n",
       "      <td>train/ID1011485656.jpg</td>\n",
       "      <td>2015/9/4</td>\n",
       "      <td>Tas</td>\n",
       "      <td>Ryegrass_Clover</td>\n",
       "      <td>0.62</td>\n",
       "      <td>4.6667</td>\n",
       "      <td>Dry_Total_g</td>\n",
       "      <td>48.2735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID1011485656__GDM_g</td>\n",
       "      <td>train/ID1011485656.jpg</td>\n",
       "      <td>2015/9/4</td>\n",
       "      <td>Tas</td>\n",
       "      <td>Ryegrass_Clover</td>\n",
       "      <td>0.62</td>\n",
       "      <td>4.6667</td>\n",
       "      <td>GDM_g</td>\n",
       "      <td>16.2750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    sample_id              image_path Sampling_Date State  \\\n",
       "0  ID1011485656__Dry_Clover_g  train/ID1011485656.jpg      2015/9/4   Tas   \n",
       "1    ID1011485656__Dry_Dead_g  train/ID1011485656.jpg      2015/9/4   Tas   \n",
       "2   ID1011485656__Dry_Green_g  train/ID1011485656.jpg      2015/9/4   Tas   \n",
       "3   ID1011485656__Dry_Total_g  train/ID1011485656.jpg      2015/9/4   Tas   \n",
       "4         ID1011485656__GDM_g  train/ID1011485656.jpg      2015/9/4   Tas   \n",
       "\n",
       "           Species  Pre_GSHH_NDVI  Height_Ave_cm   target_name   target  \n",
       "0  Ryegrass_Clover           0.62         4.6667  Dry_Clover_g   0.0000  \n",
       "1  Ryegrass_Clover           0.62         4.6667    Dry_Dead_g  31.9984  \n",
       "2  Ryegrass_Clover           0.62         4.6667   Dry_Green_g  16.2751  \n",
       "3  Ryegrass_Clover           0.62         4.6667   Dry_Total_g  48.2735  \n",
       "4  Ryegrass_Clover           0.62         4.6667         GDM_g  16.2750  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(TRAIN_CSV)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T09:05:25.115587Z",
     "iopub.status.busy": "2025-10-29T09:05:25.115375Z",
     "iopub.status.idle": "2025-10-29T09:05:25.124016Z",
     "shell.execute_reply": "2025-10-29T09:05:25.123163Z",
     "shell.execute_reply.started": "2025-10-29T09:05:25.115572Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DatasetCS(Dataset):\n",
    "    def __init__(self, df, images_dir, transform=None, is_test=False):\n",
    "        self.df = df\n",
    "        self.images_dir = images_dir\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "        \n",
    "        if not is_test:\n",
    "            self.target_mapping = {\n",
    "                'Dry_Green_g': 0, 'Dry_Dead_g': 1, 'Dry_Clover_g': 2,\n",
    "                'GDM_g': 3, 'Dry_Total_g': 4\n",
    "            }\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image_path = os.path.join(self.images_dir, row['image_path'])\n",
    "        \n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        if self.is_test:\n",
    "            return image, row['sample_id']\n",
    "        else:\n",
    "            target_value = row['target']\n",
    "            target_type = self.target_mapping[row['target_name']]\n",
    "            return image, torch.tensor(target_value, dtype=torch.float32), target_type\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((500, 250)),\n",
    "    transforms.RandomHorizontalFlip(p=0.3),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((500, 250)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T09:05:25.125618Z",
     "iopub.status.busy": "2025-10-29T09:05:25.125407Z",
     "iopub.status.idle": "2025-10-29T09:05:25.139841Z",
     "shell.execute_reply": "2025-10-29T09:05:25.139029Z",
     "shell.execute_reply.started": "2025-10-29T09:05:25.125604Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ResNet34(nn.Module):\n",
    "    def __init__(self, num_targets=5, hidden1=512, hidden2=256, head_hidden=128, \n",
    "                 dropout_shared=0.3, dropout_head=0.2):\n",
    "        super(ResNet34, self).__init__()\n",
    "        self.backbone = resnet34(weights=None)\n",
    "        self.backbone.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        \n",
    "        in_features = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Identity()\n",
    "        \n",
    "        self.shared_features = nn.Sequential(\n",
    "            nn.Linear(in_features, hidden1),\n",
    "            nn.BatchNorm1d(hidden1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_shared),\n",
    "            nn.Linear(hidden1, hidden2),\n",
    "            nn.BatchNorm1d(hidden2),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        self.heads = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Dropout(dropout_head),\n",
    "                nn.Linear(hidden2, head_hidden),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(head_hidden, 1)\n",
    "            ) for _ in range(num_targets)\n",
    "        ])\n",
    "        \n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x, target_type=None):\n",
    "        features = self.backbone(x)\n",
    "        shared_out = self.shared_features(features)\n",
    "        \n",
    "        if target_type is not None:\n",
    "            outputs = []\n",
    "            for i, t_type in enumerate(target_type):\n",
    "                outputs.append(self.heads[t_type](shared_out[i].unsqueeze(0)))\n",
    "            return torch.cat(outputs, dim=0)\n",
    "        else:\n",
    "            all_outputs = [head(shared_out) for head in self.heads]\n",
    "            return torch.cat(all_outputs, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T09:05:25.140763Z",
     "iopub.status.busy": "2025-10-29T09:05:25.140516Z",
     "iopub.status.idle": "2025-10-29T09:05:25.157980Z",
     "shell.execute_reply": "2025-10-29T09:05:25.157146Z",
     "shell.execute_reply.started": "2025-10-29T09:05:25.140747Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=15, \n",
    "                scheduler=None, verbose=True, return_best_val_loss=False):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for images, targets, target_types in train_loader:\n",
    "            images = images.to(device)\n",
    "            targets = targets.to(device)\n",
    "            target_types = target_types.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images, target_types).squeeze()\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, targets, target_types in val_loader:\n",
    "                images = images.to(device)\n",
    "                targets = targets.to(device)\n",
    "                target_types = target_types.to(device)\n",
    "                \n",
    "                outputs = model(images, target_types).squeeze()\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "        \n",
    "        if scheduler is not None:\n",
    "            scheduler.step(val_loss)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "    \n",
    "    if return_best_val_loss:\n",
    "        return train_losses, val_losses, best_val_loss\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T09:05:25.158925Z",
     "iopub.status.busy": "2025-10-29T09:05:25.158738Z",
     "iopub.status.idle": "2025-10-29T09:05:25.179873Z",
     "shell.execute_reply": "2025-10-29T09:05:25.179127Z",
     "shell.execute_reply.started": "2025-10-29T09:05:25.158911Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train length: 1428\n",
      "val length: 357\n"
     ]
    }
   ],
   "source": [
    "train_indices, val_indices = train_test_split(\n",
    "    range(len(train)), \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=train['target_name']\n",
    ")\n",
    "\n",
    "train_subset = train.iloc[train_indices].reset_index(drop=True)\n",
    "val_subset = train.iloc[val_indices].reset_index(drop=True)\n",
    "\n",
    "print(f\"train length: {len(train_subset)}\")\n",
    "print(f\"val length: {len(val_subset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T09:05:25.181186Z",
     "iopub.status.busy": "2025-10-29T09:05:25.180938Z",
     "iopub.status.idle": "2025-10-29T09:05:25.192768Z",
     "shell.execute_reply": "2025-10-29T09:05:25.191945Z",
     "shell.execute_reply.started": "2025-10-29T09:05:25.181167Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = DatasetCS(train_subset, DATA_DIR, transform=train_transform)\n",
    "val_dataset = DatasetCS(val_subset, DATA_DIR, transform=val_transform)\n",
    "\n",
    "# Adjust num_workers for local runs (0 for local to avoid multiprocessing issues, 2 for Kaggle)\n",
    "# Using 0 for local runs avoids DataLoader worker errors on macOS/Windows\n",
    "num_workers = 0 if not IS_KAGGLE else 2\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=num_workers)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization with Optuna\n",
    "\n",
    "Define an objective function for Optuna that creates and trains a model with different hyperparameters. Optuna will search for the best combination of learning rate, batch size, hidden layer sizes, dropout rates, and optimizer settings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \"\"\"Optuna objective function to optimize hyperparameters\"\"\"\n",
    "    \n",
    "    # Suggest hyperparameters\n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [8, 16, 32])\n",
    "    hidden1 = trial.suggest_categorical('hidden1', [256, 512, 768])\n",
    "    hidden2 = trial.suggest_categorical('hidden2', [128, 256, 512])\n",
    "    head_hidden = trial.suggest_categorical('head_hidden', [64, 128, 256])\n",
    "    dropout_shared = trial.suggest_float('dropout_shared', 0.1, 0.5)\n",
    "    dropout_head = trial.suggest_float('dropout_head', 0.1, 0.4)\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'AdamW', 'RMSprop'])\n",
    "    \n",
    "    # Adjust num_workers for local runs (0 for local to avoid multiprocessing issues, 2 for Kaggle)\n",
    "    num_workers_opt = 0 if not IS_KAGGLE else 2\n",
    "    \n",
    "    # Create data loaders with new batch size\n",
    "    train_loader_opt = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        num_workers=num_workers_opt\n",
    "    )\n",
    "    val_loader_opt = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=num_workers_opt\n",
    "    )\n",
    "    \n",
    "    # Create model with hyperparameters\n",
    "    model = ResNet34(\n",
    "        num_targets=5,\n",
    "        hidden1=hidden1,\n",
    "        hidden2=hidden2,\n",
    "        head_hidden=head_hidden,\n",
    "        dropout_shared=dropout_shared,\n",
    "        dropout_head=dropout_head\n",
    "    ).to(device)\n",
    "    \n",
    "    # Create optimizer\n",
    "    if optimizer_name == 'Adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    elif optimizer_name == 'AdamW':\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        optimizer = torch.optim.RMSprop(model.parameters(), lr=lr)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, patience=2, factor=0.5\n",
    "    )\n",
    "    \n",
    "    # Train for a few epochs (fewer for optimization)\n",
    "    _, _, best_val_loss = train_model(\n",
    "        model,\n",
    "        train_loader_opt,\n",
    "        val_loader_opt,\n",
    "        criterion,\n",
    "        optimizer,\n",
    "        num_epochs=5,  # Reduced epochs for faster optimization\n",
    "        scheduler=scheduler,\n",
    "        verbose=False,\n",
    "        return_best_val_loss=True\n",
    "    )\n",
    "    \n",
    "    return best_val_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Hyperparameter Optimization\n",
    "\n",
    "Create an Optuna study and run optimization trials. This will search for the best hyperparameters by training multiple models with different configurations and comparing their validation losses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-29 10:53:51,368] A new study created in memory with name: biomass_optimization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting hyperparameter optimization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Create Optuna study\n",
    "study = optuna.create_study(\n",
    "    direction='minimize',\n",
    "    study_name='biomass_optimization',\n",
    "    pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=3)\n",
    ")\n",
    "\n",
    "# Run optimization (adjust n_trials based on available time)\n",
    "print(\"Starting hyperparameter optimization...\")\n",
    "study.optimize(objective, n_trials=20, show_progress_bar=True)\n",
    "\n",
    "print(\"\\nBest trial:\")\n",
    "trial = study.best_trial\n",
    "print(f\"  Value (best validation loss): {trial.value:.4f}\")\n",
    "print(\"\\n  Params:\")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "    \n",
    "# Store best parameters\n",
    "best_params = trial.params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Optimization Results\n",
    "\n",
    "Plot the optimization history and parameter importance to understand which hyperparameters have the most impact on model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot optimization history\n",
    "fig1 = optuna.visualization.plot_optimization_history(study)\n",
    "fig1.show()\n",
    "\n",
    "# Plot parameter importance\n",
    "fig2 = optuna.visualization.plot_param_importances(study)\n",
    "fig2.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Final Model with Best Hyperparameters\n",
    "\n",
    "Train the model using the optimized hyperparameters found by Optuna. This should yield better performance than the manually selected hyperparameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders with best batch size\n",
    "best_batch_size = best_params['batch_size']\n",
    "# Adjust num_workers for local runs\n",
    "num_workers_best = 0 if not IS_KAGGLE else 2\n",
    "train_loader_best = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=best_batch_size, \n",
    "    shuffle=True, \n",
    "    num_workers=num_workers_best\n",
    ")\n",
    "val_loader_best = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=best_batch_size, \n",
    "    shuffle=False, \n",
    "    num_workers=num_workers_best\n",
    ")\n",
    "\n",
    "# Create model with best hyperparameters\n",
    "model_best = ResNet34(\n",
    "    num_targets=5,\n",
    "    hidden1=best_params['hidden1'],\n",
    "    hidden2=best_params['hidden2'],\n",
    "    head_hidden=best_params['head_hidden'],\n",
    "    dropout_shared=best_params['dropout_shared'],\n",
    "    dropout_head=best_params['dropout_head']\n",
    ").to(device)\n",
    "\n",
    "# Create optimizer with best learning rate\n",
    "if best_params['optimizer'] == 'Adam':\n",
    "    optimizer_best = torch.optim.Adam(model_best.parameters(), lr=best_params['lr'])\n",
    "elif best_params['optimizer'] == 'AdamW':\n",
    "    optimizer_best = torch.optim.AdamW(model_best.parameters(), lr=best_params['lr'])\n",
    "else:\n",
    "    optimizer_best = torch.optim.RMSprop(model_best.parameters(), lr=best_params['lr'])\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "scheduler_best = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer_best, patience=3, factor=0.5\n",
    ")\n",
    "\n",
    "# Train with best hyperparameters\n",
    "train_losses_best, val_losses_best = train_model(\n",
    "    model_best,\n",
    "    train_loader_best,\n",
    "    val_loader_best,\n",
    "    criterion,\n",
    "    optimizer_best,\n",
    "    num_epochs=15,  # Full training with best hyperparameters\n",
    "    scheduler=scheduler_best\n",
    ")\n",
    "\n",
    "# Store the best model for inference\n",
    "model = model_best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T09:05:25.195177Z",
     "iopub.status.busy": "2025-10-29T09:05:25.194907Z",
     "iopub.status.idle": "2025-10-29T09:08:10.348013Z",
     "shell.execute_reply": "2025-10-29T09:08:10.347107Z",
     "shell.execute_reply.started": "2025-10-29T09:05:25.195157Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Original training without optimization (for comparison)\n",
    "# Uncomment below to train with default hyperparameters instead of optimized ones\n",
    "\n",
    "# model = ResNet34(num_targets=5).to(device)\n",
    "# criterion = nn.MSELoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, factor=0.5)\n",
    "# train_losses, val_losses = train_model(\n",
    "#     model,\n",
    "#     train_loader,\n",
    "#     val_loader,\n",
    "#     criterion,\n",
    "#     optimizer,\n",
    "#     num_epochs=3\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T09:08:10.349377Z",
     "iopub.status.busy": "2025-10-29T09:08:10.349061Z",
     "iopub.status.idle": "2025-10-29T09:08:10.526041Z",
     "shell.execute_reply": "2025-10-29T09:08:10.525345Z",
     "shell.execute_reply.started": "2025-10-29T09:08:10.349344Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.plot(train_losses_best, label='Train Loss (Optimized)')\n",
    "plt.plot(val_losses_best, label='Val Loss (Optimized)')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training History with Optimized Hyperparameters')\n",
    "\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T09:08:10.527136Z",
     "iopub.status.busy": "2025-10-29T09:08:10.526878Z",
     "iopub.status.idle": "2025-10-29T09:08:10.934527Z",
     "shell.execute_reply": "2025-10-29T09:08:10.933545Z",
     "shell.execute_reply.started": "2025-10-29T09:08:10.527118Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(TEST_CSV)\n",
    "test_dataset = DatasetCS(test, DATA_DIR, transform=val_transform, is_test=True)\n",
    "# Adjust num_workers for local runs (0 for local to avoid multiprocessing issues, 2 for Kaggle)\n",
    "num_workers = 0 if not IS_KAGGLE else 2\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "model.eval()\n",
    "predictions = []\n",
    "sample_ids = []\n",
    "\n",
    "target_mapping = {\n",
    "    'Dry_Green_g': 0, 'Dry_Dead_g': 1, 'Dry_Clover_g': 2,\n",
    "    'GDM_g': 3, 'Dry_Total_g': 4\n",
    "}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, batch_sample_ids in test_loader:\n",
    "        images = images.to(device)\n",
    "        batch_outputs = model(images)\n",
    "        \n",
    "        for i, sample_id in enumerate(batch_sample_ids):\n",
    "            row = test[test['sample_id'] == sample_id].iloc[0]\n",
    "            target_idx = target_mapping[row['target_name']]\n",
    "            prediction = batch_outputs[i, target_idx].item()\n",
    "            predictions.append(prediction)\n",
    "            sample_ids.append(sample_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T09:08:10.936600Z",
     "iopub.status.busy": "2025-10-29T09:08:10.935765Z",
     "iopub.status.idle": "2025-10-29T09:08:10.947388Z",
     "shell.execute_reply": "2025-10-29T09:08:10.946687Z",
     "shell.execute_reply.started": "2025-10-29T09:08:10.936560Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "    'sample_id': sample_ids,\n",
    "    'target': predictions\n",
    "})\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 14254895,
     "sourceId": 112509,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
