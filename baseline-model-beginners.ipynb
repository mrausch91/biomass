{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "Import all required packages for the biomass prediction model including PyTorch for deep learning, pandas for data handling, PIL for image processing, and scikit-learn for data splitting.\n",
    "\n",
    "**Note:** If Optuna is not installed, run: `pip install optuna optuna-dashboard` (or `!pip install optuna` in a notebook cell)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Image Sizes in Dataset\n",
    "\n",
    "Define a function to scan through image directories and analyze the distribution of image dimensions. This helps understand the dataset structure and ensures proper image sizing for the model. Displays results in a formatted table showing width, height, and quantity of each size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ Installing packages from requirements.txt...\n",
      "âœ… All packages installed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Install required packages from requirements.txt\n",
    "# This ensures all dependencies are installed before running the notebook\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def install_requirements():\n",
    "    \"\"\"Install packages from requirements.txt\"\"\"\n",
    "    try:\n",
    "        # Try to read requirements.txt\n",
    "        with open('requirements.txt', 'r') as f:\n",
    "            requirements = f.read()\n",
    "        print(\"ğŸ“¦ Installing packages from requirements.txt...\")\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', '-r', 'requirements.txt'])\n",
    "        print(\"âœ… All packages installed successfully!\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"âš ï¸  requirements.txt not found. Installing core packages individually...\")\n",
    "        packages = [\n",
    "            'torch>=2.0.0',\n",
    "            'torchvision>=0.15.0',\n",
    "            'numpy>=1.21.0',\n",
    "            'pandas>=1.3.0',\n",
    "            'Pillow>=9.0.0',\n",
    "            'scikit-learn>=1.0.0',\n",
    "            'matplotlib>=3.5.0',\n",
    "            'seaborn>=0.11.0',\n",
    "            'optuna>=3.0.0'\n",
    "        ]\n",
    "        for package in packages:\n",
    "            subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', package])\n",
    "        print(\"âœ… Core packages installed!\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error installing packages: {e}\")\n",
    "        print(\"Please install packages manually or check your internet connection.\")\n",
    "\n",
    "# Install packages\n",
    "install_requirements()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Configuration\n",
    "\n",
    "Detect whether running on Kaggle or locally and set up data paths accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŸ¢ Running locally\n",
      "Data directory: .\n",
      "Train CSV: ./train.csv\n",
      "Test CSV: ./test.csv\n"
     ]
    }
   ],
   "source": [
    "# Detect environment and set data paths\n",
    "import os\n",
    "\n",
    "# Check if running on Kaggle\n",
    "IS_KAGGLE = os.path.exists('/kaggle') or os.getenv('KAGGLE_KERNEL_TYPE') is not None\n",
    "\n",
    "if IS_KAGGLE:\n",
    "    # Kaggle paths\n",
    "    DATA_DIR = '/kaggle/input/csiro-biomass'\n",
    "    TRAIN_DIR = '/kaggle/input/csiro-biomass/train'\n",
    "    TEST_DIR = '/kaggle/input/csiro-biomass/test'\n",
    "    TRAIN_CSV = '/kaggle/input/csiro-biomass/train.csv'\n",
    "    TEST_CSV = '/kaggle/input/csiro-biomass/test.csv'\n",
    "    print(\"ğŸ”µ Running on Kaggle\")\n",
    "else:\n",
    "    # Local paths - adjust these to match your local data structure\n",
    "    # Assuming data is in ./data/ directory relative to notebook\n",
    "    DATA_DIR = '.'\n",
    "    TRAIN_DIR = './train'\n",
    "    TEST_DIR = './test'\n",
    "    TRAIN_CSV = './train.csv'\n",
    "    TEST_CSV = './test.csv'\n",
    "    print(\"ğŸŸ¢ Running locally\")\n",
    "    \n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Train CSV: {TRAIN_CSV}\")\n",
    "print(f\"Test CSV: {TEST_CSV}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T09:05:24.754198Z",
     "iopub.status.busy": "2025-10-29T09:05:24.753538Z",
     "iopub.status.idle": "2025-10-29T09:05:24.760844Z",
     "shell.execute_reply": "2025-10-29T09:05:24.759754Z",
     "shell.execute_reply.started": "2025-10-29T09:05:24.754167Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/martonrausch/Documents/biomass/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "import optuna\n",
    "\n",
    "from PIL import Image\n",
    "from collections import defaultdict\n",
    "from torchvision.models import resnet34\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load training data\n",
    "\n",
    "Load the training CSV file which contains metadata about the images including image paths, target values, and target names (biomass types).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-29T09:05:24.762538Z",
     "iopub.status.busy": "2025-10-29T09:05:24.762206Z",
     "iopub.status.idle": "2025-10-29T09:05:25.094531Z",
     "shell.execute_reply": "2025-10-29T09:05:25.093593Z",
     "shell.execute_reply.started": "2025-10-29T09:05:24.762517Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“‚ Folder: ./train\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚  Width (px)  â”‚ Height (px)  â”‚Quantityâ”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ 2000          â”‚ 1000          â”‚ 357     â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "ğŸ“‚ Folder: ./test\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚  Width (px)  â”‚ Height (px)  â”‚Quantityâ”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ 2000          â”‚ 1000          â”‚ 1       â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
     ]
    }
   ],
   "source": [
    "def get_unique_sizes(directory):\n",
    "    size_counts = defaultdict(int)\n",
    "    if not os.path.exists(directory):\n",
    "        return size_counts\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.png', '.jpg', '.jpeg', 'JPG')):\n",
    "                try:\n",
    "                    with Image.open(os.path.join(root, file)) as img:\n",
    "                        size = img.size\n",
    "                        size_counts[size] += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"Error {file}: {e}\")\n",
    "\n",
    "    return size_counts\n",
    "\n",
    "folders = [\n",
    "    TRAIN_DIR,\n",
    "    TEST_DIR,\n",
    "]\n",
    "\n",
    "for folder in folders:\n",
    "    print(f\"\\nğŸ“‚ Folder: {folder}\")\n",
    "    if not os.path.exists(folder):\n",
    "        print(f\"âš ï¸  Directory does not exist: {folder}\")\n",
    "        continue\n",
    "    sizes = get_unique_sizes(folder)\n",
    "\n",
    "    if not sizes:\n",
    "        print(\"No images or mistake in code\")\n",
    "        continue\n",
    "    \n",
    "    sorted_sizes = sorted(sizes.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    print(\"â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\")\n",
    "    print(\"â”‚  Width (px)  â”‚ Height (px)  â”‚Quantityâ”‚\")\n",
    "    print(\"â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\")\n",
    "    for (w, h), count in sorted_sizes:\n",
    "        print(f\"â”‚ {w:<13} â”‚ {h:<13} â”‚ {count:<7} â”‚\")\n",
    "    print(\"â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Dataset Class and Data Transforms\n",
    "\n",
    "Define a custom PyTorch Dataset class that:\n",
    "- Loads images from paths in the dataframe\n",
    "- Applies transformations (resizing, augmentation for training)\n",
    "- Maps different target types (biomass categories) to indices\n",
    "- Returns images with their corresponding target values and types\n",
    "\n",
    "Also defines two transform pipelines:\n",
    "- `train_transform`: Includes data augmentation (random flips, color jitter) for training\n",
    "- `val_transform`: Only resizing and normalization for validation/testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture: Multi-Head ResNet34\n",
    "\n",
    "Define a ResNet34-based model with multiple output heads (one for each target type):\n",
    "- Uses ResNet34 as the feature extraction backbone\n",
    "- Extracts shared features through fully connected layers\n",
    "- Has 5 separate heads (one for each biomass type: Dry_Green_g, Dry_Dead_g, Dry_Clover_g, GDM_g, Dry_Total_g)\n",
    "- Each head outputs a single value (regression prediction)\n",
    "- The model selects which head to use based on the target_type during training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Function\n",
    "\n",
    "Define the model training loop that:\n",
    "- Trains the model for specified number of epochs\n",
    "- Iterates through training batches, computes loss, and updates weights\n",
    "- Evaluates on validation set after each epoch\n",
    "- Tracks and returns training and validation losses\n",
    "- Uses the appropriate model head based on target_type for each sample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data into Train and Validation Sets\n",
    "\n",
    "Split the training data into train (80%) and validation (20%) sets while maintaining stratification by target_name to ensure balanced distribution of biomass types in both sets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Datasets and Data Loaders\n",
    "\n",
    "Instantiate the training and validation datasets with their respective transforms, then create DataLoader objects for efficient batch loading during training. Also determine and set the computation device (GPU if available, else CPU).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Model and Train\n",
    "\n",
    "Create the ResNet34 model instance, define loss function (MSE for regression), optimizer (Adam), and learning rate scheduler. Then train the model for the specified number of epochs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Training History\n",
    "\n",
    "Plot the training and validation loss curves to visualize model performance over epochs and check for overfitting or convergence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Predictions on Test Set\n",
    "\n",
    "Load the test dataset and make predictions:\n",
    "- Load test CSV and create test dataset with validation transforms\n",
    "- Run inference on test images\n",
    "- For each sample, select the appropriate output head based on the target_name\n",
    "- Collect predictions and corresponding sample IDs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Submission File\n",
    "\n",
    "Format the predictions into a submission CSV file with sample_id and target columns, and save it for competition submission.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T09:05:25.095578Z",
     "iopub.status.busy": "2025-10-29T09:05:25.095309Z",
     "iopub.status.idle": "2025-10-29T09:05:25.113935Z",
     "shell.execute_reply": "2025-10-29T09:05:25.113165Z",
     "shell.execute_reply.started": "2025-10-29T09:05:25.095553Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>image_path</th>\n",
       "      <th>Sampling_Date</th>\n",
       "      <th>State</th>\n",
       "      <th>Species</th>\n",
       "      <th>Pre_GSHH_NDVI</th>\n",
       "      <th>Height_Ave_cm</th>\n",
       "      <th>target_name</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID1011485656__Dry_Clover_g</td>\n",
       "      <td>train/ID1011485656.jpg</td>\n",
       "      <td>2015/9/4</td>\n",
       "      <td>Tas</td>\n",
       "      <td>Ryegrass_Clover</td>\n",
       "      <td>0.62</td>\n",
       "      <td>4.6667</td>\n",
       "      <td>Dry_Clover_g</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID1011485656__Dry_Dead_g</td>\n",
       "      <td>train/ID1011485656.jpg</td>\n",
       "      <td>2015/9/4</td>\n",
       "      <td>Tas</td>\n",
       "      <td>Ryegrass_Clover</td>\n",
       "      <td>0.62</td>\n",
       "      <td>4.6667</td>\n",
       "      <td>Dry_Dead_g</td>\n",
       "      <td>31.9984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID1011485656__Dry_Green_g</td>\n",
       "      <td>train/ID1011485656.jpg</td>\n",
       "      <td>2015/9/4</td>\n",
       "      <td>Tas</td>\n",
       "      <td>Ryegrass_Clover</td>\n",
       "      <td>0.62</td>\n",
       "      <td>4.6667</td>\n",
       "      <td>Dry_Green_g</td>\n",
       "      <td>16.2751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID1011485656__Dry_Total_g</td>\n",
       "      <td>train/ID1011485656.jpg</td>\n",
       "      <td>2015/9/4</td>\n",
       "      <td>Tas</td>\n",
       "      <td>Ryegrass_Clover</td>\n",
       "      <td>0.62</td>\n",
       "      <td>4.6667</td>\n",
       "      <td>Dry_Total_g</td>\n",
       "      <td>48.2735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID1011485656__GDM_g</td>\n",
       "      <td>train/ID1011485656.jpg</td>\n",
       "      <td>2015/9/4</td>\n",
       "      <td>Tas</td>\n",
       "      <td>Ryegrass_Clover</td>\n",
       "      <td>0.62</td>\n",
       "      <td>4.6667</td>\n",
       "      <td>GDM_g</td>\n",
       "      <td>16.2750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    sample_id              image_path Sampling_Date State  \\\n",
       "0  ID1011485656__Dry_Clover_g  train/ID1011485656.jpg      2015/9/4   Tas   \n",
       "1    ID1011485656__Dry_Dead_g  train/ID1011485656.jpg      2015/9/4   Tas   \n",
       "2   ID1011485656__Dry_Green_g  train/ID1011485656.jpg      2015/9/4   Tas   \n",
       "3   ID1011485656__Dry_Total_g  train/ID1011485656.jpg      2015/9/4   Tas   \n",
       "4         ID1011485656__GDM_g  train/ID1011485656.jpg      2015/9/4   Tas   \n",
       "\n",
       "           Species  Pre_GSHH_NDVI  Height_Ave_cm   target_name   target  \n",
       "0  Ryegrass_Clover           0.62         4.6667  Dry_Clover_g   0.0000  \n",
       "1  Ryegrass_Clover           0.62         4.6667    Dry_Dead_g  31.9984  \n",
       "2  Ryegrass_Clover           0.62         4.6667   Dry_Green_g  16.2751  \n",
       "3  Ryegrass_Clover           0.62         4.6667   Dry_Total_g  48.2735  \n",
       "4  Ryegrass_Clover           0.62         4.6667         GDM_g  16.2750  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(TRAIN_CSV)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T09:05:25.115587Z",
     "iopub.status.busy": "2025-10-29T09:05:25.115375Z",
     "iopub.status.idle": "2025-10-29T09:05:25.124016Z",
     "shell.execute_reply": "2025-10-29T09:05:25.123163Z",
     "shell.execute_reply.started": "2025-10-29T09:05:25.115572Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DatasetCS(Dataset):\n",
    "    def __init__(self, df, images_dir, transform=None, is_test=False):\n",
    "        self.df = df\n",
    "        self.images_dir = images_dir\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "        \n",
    "        if not is_test:\n",
    "            self.target_mapping = {\n",
    "                'Dry_Green_g': 0, 'Dry_Dead_g': 1, 'Dry_Clover_g': 2,\n",
    "                'GDM_g': 3, 'Dry_Total_g': 4\n",
    "            }\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image_path = os.path.join(self.images_dir, row['image_path'])\n",
    "        \n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        if self.is_test:\n",
    "            return image, row['sample_id']\n",
    "        else:\n",
    "            target_value = row['target']\n",
    "            target_type = self.target_mapping[row['target_name']]\n",
    "            return image, torch.tensor(target_value, dtype=torch.float32), target_type\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((500, 250)),\n",
    "    transforms.RandomHorizontalFlip(p=0.3),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((500, 250)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T09:05:25.125618Z",
     "iopub.status.busy": "2025-10-29T09:05:25.125407Z",
     "iopub.status.idle": "2025-10-29T09:05:25.139841Z",
     "shell.execute_reply": "2025-10-29T09:05:25.139029Z",
     "shell.execute_reply.started": "2025-10-29T09:05:25.125604Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ResNet34(nn.Module):\n",
    "    def __init__(self, num_targets=5, hidden1=512, hidden2=256, head_hidden=128, \n",
    "                 dropout_shared=0.3, dropout_head=0.2):\n",
    "        super(ResNet34, self).__init__()\n",
    "        self.backbone = resnet34(weights=None)\n",
    "        self.backbone.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        \n",
    "        in_features = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Identity()\n",
    "        \n",
    "        self.shared_features = nn.Sequential(\n",
    "            nn.Linear(in_features, hidden1),\n",
    "            nn.BatchNorm1d(hidden1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_shared),\n",
    "            nn.Linear(hidden1, hidden2),\n",
    "            nn.BatchNorm1d(hidden2),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        self.heads = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Dropout(dropout_head),\n",
    "                nn.Linear(hidden2, head_hidden),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(head_hidden, 1)\n",
    "            ) for _ in range(num_targets)\n",
    "        ])\n",
    "        \n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x, target_type=None):\n",
    "        features = self.backbone(x)\n",
    "        shared_out = self.shared_features(features)\n",
    "        \n",
    "        if target_type is not None:\n",
    "            outputs = []\n",
    "            for i, t_type in enumerate(target_type):\n",
    "                outputs.append(self.heads[t_type](shared_out[i].unsqueeze(0)))\n",
    "            return torch.cat(outputs, dim=0)\n",
    "        else:\n",
    "            all_outputs = [head(shared_out) for head in self.heads]\n",
    "            return torch.cat(all_outputs, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T09:05:25.140763Z",
     "iopub.status.busy": "2025-10-29T09:05:25.140516Z",
     "iopub.status.idle": "2025-10-29T09:05:25.157980Z",
     "shell.execute_reply": "2025-10-29T09:05:25.157146Z",
     "shell.execute_reply.started": "2025-10-29T09:05:25.140747Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=15, \n",
    "                scheduler=None, verbose=True, return_best_val_loss=False):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for images, targets, target_types in train_loader:\n",
    "            images = images.to(device)\n",
    "            targets = targets.to(device)\n",
    "            target_types = target_types.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images, target_types).squeeze()\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, targets, target_types in val_loader:\n",
    "                images = images.to(device)\n",
    "                targets = targets.to(device)\n",
    "                target_types = target_types.to(device)\n",
    "                \n",
    "                outputs = model(images, target_types).squeeze()\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "        \n",
    "        if scheduler is not None:\n",
    "            scheduler.step(val_loss)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "    \n",
    "    if return_best_val_loss:\n",
    "        return train_losses, val_losses, best_val_loss\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T09:05:25.158925Z",
     "iopub.status.busy": "2025-10-29T09:05:25.158738Z",
     "iopub.status.idle": "2025-10-29T09:05:25.179873Z",
     "shell.execute_reply": "2025-10-29T09:05:25.179127Z",
     "shell.execute_reply.started": "2025-10-29T09:05:25.158911Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train length: 1428\n",
      "val length: 357\n"
     ]
    }
   ],
   "source": [
    "train_indices, val_indices = train_test_split(\n",
    "    range(len(train)), \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=train['target_name']\n",
    ")\n",
    "\n",
    "train_subset = train.iloc[train_indices].reset_index(drop=True)\n",
    "val_subset = train.iloc[val_indices].reset_index(drop=True)\n",
    "\n",
    "print(f\"train length: {len(train_subset)}\")\n",
    "print(f\"val length: {len(val_subset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T09:05:25.181186Z",
     "iopub.status.busy": "2025-10-29T09:05:25.180938Z",
     "iopub.status.idle": "2025-10-29T09:05:25.192768Z",
     "shell.execute_reply": "2025-10-29T09:05:25.191945Z",
     "shell.execute_reply.started": "2025-10-29T09:05:25.181167Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = DatasetCS(train_subset, DATA_DIR, transform=train_transform)\n",
    "val_dataset = DatasetCS(val_subset, DATA_DIR, transform=val_transform)\n",
    "\n",
    "# Adjust num_workers for local runs (0 for local to avoid multiprocessing issues, 2 for Kaggle)\n",
    "# Using 0 for local runs avoids DataLoader worker errors on macOS/Windows\n",
    "num_workers = 0 if not IS_KAGGLE else 2\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=num_workers)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "# Device configuration: prioritize GPU on Kaggle\n",
    "if IS_KAGGLE:\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        print(f\"âœ… GPU available: Using {device}\")\n",
    "        print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"   CUDA Version: {torch.version.cuda}\")\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        print(\"âš ï¸  WARNING: GPU not available on Kaggle!\")\n",
    "        print(\"   Please enable GPU in Kaggle Notebook Settings (Settings â†’ Accelerator â†’ GPU)\")\n",
    "        print(f\"   Falling back to: {device}\")\n",
    "else:\n",
    "    # Local: use GPU if available, otherwise CPU\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization with Optuna\n",
    "\n",
    "Define an objective function for Optuna that creates and trains a model with different hyperparameters. Optuna will search for the best combination of learning rate, batch size, hidden layer sizes, dropout rates, and optimizer settings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \"\"\"Optuna objective function to optimize hyperparameters\"\"\"\n",
    "    \n",
    "    # Suggest hyperparameters\n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [8, 16, 32])\n",
    "    hidden1 = trial.suggest_categorical('hidden1', [256, 512, 768])\n",
    "    hidden2 = trial.suggest_categorical('hidden2', [128, 256, 512])\n",
    "    head_hidden = trial.suggest_categorical('head_hidden', [64, 128, 256])\n",
    "    dropout_shared = trial.suggest_float('dropout_shared', 0.1, 0.5)\n",
    "    dropout_head = trial.suggest_float('dropout_head', 0.1, 0.4)\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'AdamW', 'RMSprop'])\n",
    "    \n",
    "    # Adjust num_workers for local runs (0 for local to avoid multiprocessing issues, 2 for Kaggle)\n",
    "    num_workers_opt = 0 if not IS_KAGGLE else 2\n",
    "    \n",
    "    # Create data loaders with new batch size\n",
    "    train_loader_opt = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        num_workers=num_workers_opt\n",
    "    )\n",
    "    val_loader_opt = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=num_workers_opt\n",
    "    )\n",
    "    \n",
    "    # Create model with hyperparameters\n",
    "    model = ResNet34(\n",
    "        num_targets=5,\n",
    "        hidden1=hidden1,\n",
    "        hidden2=hidden2,\n",
    "        head_hidden=head_hidden,\n",
    "        dropout_shared=dropout_shared,\n",
    "        dropout_head=dropout_head\n",
    "    ).to(device)\n",
    "    \n",
    "    # Create optimizer\n",
    "    if optimizer_name == 'Adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    elif optimizer_name == 'AdamW':\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        optimizer = torch.optim.RMSprop(model.parameters(), lr=lr)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, patience=2, factor=0.5\n",
    "    )\n",
    "    \n",
    "    # Train for a few epochs (fewer for optimization)\n",
    "    _, _, best_val_loss = train_model(\n",
    "        model,\n",
    "        train_loader_opt,\n",
    "        val_loader_opt,\n",
    "        criterion,\n",
    "        optimizer,\n",
    "        num_epochs=5,  # Reduced epochs for faster optimization\n",
    "        scheduler=scheduler,\n",
    "        verbose=False,\n",
    "        return_best_val_loss=True\n",
    "    )\n",
    "    \n",
    "    return best_val_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Hyperparameter Optimization\n",
    "\n",
    "Create an Optuna study and run optimization trials. This will search for the best hyperparameters by training multiple models with different configurations and comparing their validation losses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-29 10:53:51,368] A new study created in memory with name: biomass_optimization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting hyperparameter optimization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [22:26<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-10-29 11:16:17,623] Trial 0 failed with parameters: {'lr': 0.003262604873054273, 'batch_size': 32, 'hidden1': 256, 'hidden2': 256, 'head_hidden': 256, 'dropout_shared': 0.1532563247555654, 'dropout_head': 0.28294388216954375, 'optimizer': 'RMSprop'} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/martonrausch/Documents/biomass/.venv/lib/python3.13/site-packages/optuna/study/_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/var/folders/66/_pdx6cb17n3_wqrcb_3vl3480000gn/T/ipykernel_14681/1153165064.py\", line 55, in objective\n",
      "    _, _, best_val_loss = train_model(\n",
      "                          ~~~~~~~~~~~^\n",
      "        model,\n",
      "        ^^^^^^\n",
      "    ...<7 lines>...\n",
      "        return_best_val_loss=True\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/var/folders/66/_pdx6cb17n3_wqrcb_3vl3480000gn/T/ipykernel_14681/2597860744.py\", line 17, in train_model\n",
      "    outputs = model(images, target_types).squeeze()\n",
      "              ~~~~~^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/martonrausch/Documents/biomass/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/martonrausch/Documents/biomass/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/var/folders/66/_pdx6cb17n3_wqrcb_3vl3480000gn/T/ipykernel_14681/1196117100.py\", line 39, in forward\n",
      "    features = self.backbone(x)\n",
      "  File \"/Users/martonrausch/Documents/biomass/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/martonrausch/Documents/biomass/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/martonrausch/Documents/biomass/.venv/lib/python3.13/site-packages/torchvision/models/resnet.py\", line 285, in forward\n",
      "    return self._forward_impl(x)\n",
      "           ~~~~~~~~~~~~~~~~~~^^^\n",
      "  File \"/Users/martonrausch/Documents/biomass/.venv/lib/python3.13/site-packages/torchvision/models/resnet.py\", line 268, in _forward_impl\n",
      "    x = self.conv1(x)\n",
      "  File \"/Users/martonrausch/Documents/biomass/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/martonrausch/Documents/biomass/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/martonrausch/Documents/biomass/.venv/lib/python3.13/site-packages/torch/nn/modules/conv.py\", line 548, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "           ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/martonrausch/Documents/biomass/.venv/lib/python3.13/site-packages/torch/nn/modules/conv.py\", line 543, in _conv_forward\n",
      "    return F.conv2d(\n",
      "           ~~~~~~~~^\n",
      "        input, weight, bias, self.stride, self.padding, self.dilation, self.groups\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "KeyboardInterrupt\n",
      "[W 2025-10-29 11:16:17,626] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Run optimization (adjust n_trials based on available time)\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting hyperparameter optimization...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43mstudy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mBest trial:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m trial = study.best_trial\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/biomass/.venv/lib/python3.13/site-packages/optuna/study/study.py:490\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    389\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    390\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    397\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    398\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    399\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    400\u001b[39m \n\u001b[32m    401\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    488\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    489\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m490\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/biomass/.venv/lib/python3.13/site-packages/optuna/study/_optimize.py:63\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     76\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/biomass/.venv/lib/python3.13/site-packages/optuna/study/_optimize.py:160\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     frozen_trial_id = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    163\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/biomass/.venv/lib/python3.13/site-packages/optuna/study/_optimize.py:258\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    251\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    253\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    254\u001b[39m     updated_state == TrialState.FAIL\n\u001b[32m    255\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    256\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    257\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    259\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m trial._trial_id\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/biomass/.venv/lib/python3.13/site-packages/optuna/study/_optimize.py:201\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    203\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    204\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 55\u001b[39m, in \u001b[36mobjective\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m     50\u001b[39m scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n\u001b[32m     51\u001b[39m     optimizer, patience=\u001b[32m2\u001b[39m, factor=\u001b[32m0.5\u001b[39m\n\u001b[32m     52\u001b[39m )\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# Train for a few epochs (fewer for optimization)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m _, _, best_val_loss = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader_opt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_loader_opt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Reduced epochs for faster optimization\u001b[39;49;00m\n\u001b[32m     62\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_best_val_loss\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     65\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m best_val_loss\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, val_loader, criterion, optimizer, num_epochs, scheduler, verbose, return_best_val_loss)\u001b[39m\n\u001b[32m     14\u001b[39m target_types = target_types.to(device)\n\u001b[32m     16\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_types\u001b[49m\u001b[43m)\u001b[49m.squeeze()\n\u001b[32m     18\u001b[39m loss = criterion(outputs, targets)\n\u001b[32m     19\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/biomass/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/biomass/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 39\u001b[39m, in \u001b[36mResNet34.forward\u001b[39m\u001b[34m(self, x, target_type)\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, target_type=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m     shared_out = \u001b[38;5;28mself\u001b[39m.shared_features(features)\n\u001b[32m     42\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m target_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/biomass/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/biomass/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/biomass/.venv/lib/python3.13/site-packages/torchvision/models/resnet.py:285\u001b[39m, in \u001b[36mResNet.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/biomass/.venv/lib/python3.13/site-packages/torchvision/models/resnet.py:268\u001b[39m, in \u001b[36mResNet._forward_impl\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    266\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_forward_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) -> Tensor:\n\u001b[32m    267\u001b[39m     \u001b[38;5;66;03m# See note [TorchScript super()]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m268\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    269\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.bn1(x)\n\u001b[32m    270\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.relu(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/biomass/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/biomass/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/biomass/.venv/lib/python3.13/site-packages/torch/nn/modules/conv.py:548\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    547\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m548\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/biomass/.venv/lib/python3.13/site-packages/torch/nn/modules/conv.py:543\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    532\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    533\u001b[39m         F.pad(\n\u001b[32m    534\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    541\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    542\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m543\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Create Optuna study\n",
    "study = optuna.create_study(\n",
    "    direction='minimize',\n",
    "    study_name='biomass_optimization',\n",
    "    pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=3)\n",
    ")\n",
    "\n",
    "# Run optimization (adjust n_trials based on available time)\n",
    "print(\"Starting hyperparameter optimization...\")\n",
    "study.optimize(objective, n_trials=20, show_progress_bar=True)\n",
    "\n",
    "print(\"\\nBest trial:\")\n",
    "trial = study.best_trial\n",
    "print(f\"  Value (best validation loss): {trial.value:.4f}\")\n",
    "print(\"\\n  Params:\")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "    \n",
    "# Store best parameters\n",
    "best_params = trial.params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Optimization Results\n",
    "\n",
    "Plot the optimization history and parameter importance to understand which hyperparameters have the most impact on model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot optimization history\n",
    "fig1 = optuna.visualization.plot_optimization_history(study)\n",
    "fig1.show()\n",
    "\n",
    "# Plot parameter importance\n",
    "fig2 = optuna.visualization.plot_param_importances(study)\n",
    "fig2.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Final Model with Best Hyperparameters\n",
    "\n",
    "Train the model using the optimized hyperparameters found by Optuna. This should yield better performance than the manually selected hyperparameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders with best batch size\n",
    "best_batch_size = best_params['batch_size']\n",
    "# Adjust num_workers for local runs\n",
    "num_workers_best = 0 if not IS_KAGGLE else 2\n",
    "train_loader_best = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=best_batch_size, \n",
    "    shuffle=True, \n",
    "    num_workers=num_workers_best\n",
    ")\n",
    "val_loader_best = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=best_batch_size, \n",
    "    shuffle=False, \n",
    "    num_workers=num_workers_best\n",
    ")\n",
    "\n",
    "# Create model with best hyperparameters\n",
    "model_best = ResNet34(\n",
    "    num_targets=5,\n",
    "    hidden1=best_params['hidden1'],\n",
    "    hidden2=best_params['hidden2'],\n",
    "    head_hidden=best_params['head_hidden'],\n",
    "    dropout_shared=best_params['dropout_shared'],\n",
    "    dropout_head=best_params['dropout_head']\n",
    ").to(device)\n",
    "\n",
    "# Create optimizer with best learning rate\n",
    "if best_params['optimizer'] == 'Adam':\n",
    "    optimizer_best = torch.optim.Adam(model_best.parameters(), lr=best_params['lr'])\n",
    "elif best_params['optimizer'] == 'AdamW':\n",
    "    optimizer_best = torch.optim.AdamW(model_best.parameters(), lr=best_params['lr'])\n",
    "else:\n",
    "    optimizer_best = torch.optim.RMSprop(model_best.parameters(), lr=best_params['lr'])\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "scheduler_best = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer_best, patience=3, factor=0.5\n",
    ")\n",
    "\n",
    "# Train with best hyperparameters\n",
    "train_losses_best, val_losses_best = train_model(\n",
    "    model_best,\n",
    "    train_loader_best,\n",
    "    val_loader_best,\n",
    "    criterion,\n",
    "    optimizer_best,\n",
    "    num_epochs=15,  # Full training with best hyperparameters\n",
    "    scheduler=scheduler_best\n",
    ")\n",
    "\n",
    "# Store the best model for inference\n",
    "model = model_best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T09:05:25.195177Z",
     "iopub.status.busy": "2025-10-29T09:05:25.194907Z",
     "iopub.status.idle": "2025-10-29T09:08:10.348013Z",
     "shell.execute_reply": "2025-10-29T09:08:10.347107Z",
     "shell.execute_reply.started": "2025-10-29T09:05:25.195157Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Original training without optimization (for comparison)\n",
    "# Uncomment below to train with default hyperparameters instead of optimized ones\n",
    "\n",
    "# model = ResNet34(num_targets=5).to(device)\n",
    "# criterion = nn.MSELoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, factor=0.5)\n",
    "# train_losses, val_losses = train_model(\n",
    "#     model,\n",
    "#     train_loader,\n",
    "#     val_loader,\n",
    "#     criterion,\n",
    "#     optimizer,\n",
    "#     num_epochs=3\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T09:08:10.349377Z",
     "iopub.status.busy": "2025-10-29T09:08:10.349061Z",
     "iopub.status.idle": "2025-10-29T09:08:10.526041Z",
     "shell.execute_reply": "2025-10-29T09:08:10.525345Z",
     "shell.execute_reply.started": "2025-10-29T09:08:10.349344Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.plot(train_losses_best, label='Train Loss (Optimized)')\n",
    "plt.plot(val_losses_best, label='Val Loss (Optimized)')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training History with Optimized Hyperparameters')\n",
    "\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T09:08:10.527136Z",
     "iopub.status.busy": "2025-10-29T09:08:10.526878Z",
     "iopub.status.idle": "2025-10-29T09:08:10.934527Z",
     "shell.execute_reply": "2025-10-29T09:08:10.933545Z",
     "shell.execute_reply.started": "2025-10-29T09:08:10.527118Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(TEST_CSV)\n",
    "test_dataset = DatasetCS(test, DATA_DIR, transform=val_transform, is_test=True)\n",
    "# Adjust num_workers for local runs (0 for local to avoid multiprocessing issues, 2 for Kaggle)\n",
    "num_workers = 0 if not IS_KAGGLE else 2\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "model.eval()\n",
    "predictions = []\n",
    "sample_ids = []\n",
    "\n",
    "target_mapping = {\n",
    "    'Dry_Green_g': 0, 'Dry_Dead_g': 1, 'Dry_Clover_g': 2,\n",
    "    'GDM_g': 3, 'Dry_Total_g': 4\n",
    "}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, batch_sample_ids in test_loader:\n",
    "        images = images.to(device)\n",
    "        batch_outputs = model(images)\n",
    "        \n",
    "        for i, sample_id in enumerate(batch_sample_ids):\n",
    "            row = test[test['sample_id'] == sample_id].iloc[0]\n",
    "            target_idx = target_mapping[row['target_name']]\n",
    "            prediction = batch_outputs[i, target_idx].item()\n",
    "            predictions.append(prediction)\n",
    "            sample_ids.append(sample_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T09:08:10.936600Z",
     "iopub.status.busy": "2025-10-29T09:08:10.935765Z",
     "iopub.status.idle": "2025-10-29T09:08:10.947388Z",
     "shell.execute_reply": "2025-10-29T09:08:10.946687Z",
     "shell.execute_reply.started": "2025-10-29T09:08:10.936560Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "    'sample_id': sample_ids,\n",
    "    'target': predictions\n",
    "})\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 14254895,
     "sourceId": 112509,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
